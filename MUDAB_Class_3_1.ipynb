{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e3SpPNdP8As"
   },
   "source": [
    "# Class 3 - Usar ficheros desde S3\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Usar datos reales de gran tamaño almacenados en S3\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. subir datos a S3\n",
    "2. utilizar S3 como fuente de datos de Tweets \n",
    "3. contestar preguntas sobre estos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Operaciones en S3\n",
    "\n",
    "Para praticar las operaciones disponibles en S3 usaremos la consola del Learner Lab. \n",
    "\n",
    "## Buckets \n",
    "\n",
    "### Crear un bucket \n",
    "\n",
    "Para crear el bucket `bucket-de-ejemplo`, el comando es: \n",
    "\n",
    "```aws\n",
    "aws s3api create-bucket -bucket bucket-de-ejemplo\n",
    "```\n",
    "\n",
    "Con el comando \n",
    "```aws\n",
    "aws s3api list-buckets  \n",
    "```\n",
    "\n",
    "podemos comprobar que el bucket haya sido creado. \n",
    "\n",
    "&#x26A0; **Importante!** No pueden existir globalmente en S3 dos buckets con el mismo nombre, aunque sean creados por personas/cuentas diferentes. \n",
    "\n",
    "### Eliminar un bucket \n",
    "\n",
    "Para eliminar el bucket `bucket-de-ejemplo`, el comando es: \n",
    "\n",
    "```aws\n",
    "aws s3api delete-bucket -bucket bucket-de-ejemplo\n",
    "```\n",
    "\n",
    "Con el comando \n",
    "```aws\n",
    "aws s3api list-buckets  \n",
    "```\n",
    "\n",
    "podemos comprobar que el bucket haya sido eliminado. \n",
    "\n",
    "## Ficheros\n",
    "\n",
    "### Hacer el listado de ficheros en un directorio \n",
    "\n",
    "En S3 los directorios como si no existen, pero cuando dos objetos tienen un prefijo comun separado por `/` este prefijo comun se puede interpretar como un directorio. Por ejemplo, el fichero con key `test/sub/file1` y el fichero con key `test/sub/file2` comparten el mismo prefijo `test/sub`, que podemos interpretar como que en el bucket contiene un directorio `test` que contiene un directorio `sub`, que contiene dos ficheros `file1` y `file2`. \n",
    "\n",
    "Por lo tanto, imaginando que estos ficheros sean dentro de un bucket llamado `my-bucket`, podemos pedir por consola de ver el contenido del directorio `test/sub` con el siguiente comando:\n",
    "\n",
    "```\n",
    "aws s3 ls s3://my-bucket/test/sub \n",
    "```\n",
    "\n",
    "### Copiar un fichero a S3 \n",
    "\n",
    "Un fichero `my-file.txt` desde el ordenador se puede copiar a S3 con el siguiente comando:\n",
    "\n",
    "```\n",
    "aws s3 cp my-file.txt s3://my-bucket/my-dir/my-file.txt\n",
    "```\n",
    "\n",
    "También es posible copiar un fichero de S3 a S3, siempre y cuando tengamos los permisos suficientes para realizar la operación (permisos de lectura del fichero de origen, y permiso de escritura al bucket de destino).\n",
    "\n",
    "Por ejemplo el siguiente comando: \n",
    "```\n",
    "aws s3 cp s3://my-bucket/file1.txt s3://my-other-bucket/copy/file1\n",
    "```\n",
    "\n",
    "copia el fichero `file1.txt` desde el bucket `my-bucket` al bucket `my-other-bucket` con el key `copy/file1`. Esto quiere decir que el fichero `file1` estará automáticamente en un directorio `copy` que se creará sin necesidad de existir previamente. Con el comando \n",
    "\n",
    "```\n",
    "aws s3 ls s3://my-other-bucket/copy/ \n",
    "```\n",
    "confirmaremos que el fichero está en el directorio `copy` bajo el nombre `file1` \n",
    "\n",
    "### Otras operaciones sobre ficheros \"locales\"\n",
    "\n",
    "Para hacer pruebas, puede ser útil crear ficheros **desde la consola del Learner Lab**. \n",
    "\n",
    "El siguiente comando crea un fichero vacío llamado `my-file.txt`\n",
    "\n",
    "```bash\n",
    "touch my-file.txt\n",
    "```\n",
    "\n",
    "El siguiente comando imprime en pantalla el listado de ficheros en el directorio actual: \n",
    "\n",
    "```bash\n",
    "ls\n",
    "```\n",
    "\n",
    "También podemos crear un fichero con un contenido desde la consola: \n",
    "\n",
    "```bash\n",
    "echo '12345' > my-second-file.txt \n",
    "```\n",
    "\n",
    "El contenido del fichero `my-second-file.txt` será el texto `12345`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1 (en clase)\n",
    "**Usando la consola del Learner Lab**, ejecuta lo siguiente:\n",
    "\n",
    "- Crea un bucket llamado `mudab-ixxxxx` donde `ixxxxx` es tu número identificativo de estudiante\n",
    "- Crea un fichero local con el contenido `abcde` y nombre `file1.txt`\n",
    "- Copia el fichero en tu bucket, bajo el nombre `test/consola/file1`\n",
    "- Copia de nuevo el fichero, bajo el nombre `test/consola/file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/consola/`, deberían aparecer 2 ficheros\n",
    "- Borra el fichero `file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/consola/`, debería aparecer 1 fichero\n",
    "\n",
    "### Descarga en tu ordenador  \n",
    "Si quieres tener los ficheros en tu ordenador para hacer pruebas por tu cuenta, pues los objetos públicos de S3 tiene un URL desde donde descargar el objeto. La versión comprimida de los tweet está disponible aquí: https://mudab-2025-big-data.s3.us-east-1.amazonaws.com/twitter-data-compressed/twitter-data-from-eurovision-2018-splits.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nMLMr9Ks5_D"
   },
   "source": [
    "# Parte 2: S3 de manera programática \n",
    "\n",
    "En esta parte, añadiremos el acceso a S3 como sistema de ficheros a través de una librería llamada [s3fs](https://s3fs.readthedocs.io/en/latest/). Esta librería nos permite acceder a S3 como si fuera un sistema de ficheros local. \n",
    "\n",
    "Aunque el acceso a S3 de manera programatica requiera credenciales, la ejecución en entorno SageMaker nos permite omitir este paso de configuración, y tener un código más sencillo. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "bucket='mudab-2025-big-data'\n",
    "data_key = 'twitter-data/Eurovision-00.json'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones sobre ficheros con la librería s3fs \n",
    "\n",
    "La librería s3fs es un ejemplo de acceso programático a los servicios de AWS. En este caso, nos permite acceder a S3 y hacer operaciones utilizando Python. Las operaciones sobre ficheros explicadas arriba se traducen en pequeños *snippets* de código.\n",
    "\n",
    "#### Hacer el listado de ficheros en un directorio\n",
    "\n",
    "```python\n",
    "s3.ls(\"my-bucket/test/sub\")\n",
    "```\n",
    "\n",
    "#### Copiar un fichero a S3\n",
    "\n",
    "```python\n",
    "s3.put(\"my-file.txt\", \"s3://my-bucket/my-dir/my-file.txt\")\n",
    "```\n",
    "\n",
    "#### Borrar un fichero desde S3 \n",
    "\n",
    "```python\n",
    "s3.rm(\"my-bucket/my-dir/my-file.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Actividad 2 (en clase)\n",
    "\n",
    "Primero, crea en tu ordenador un fichero local con el contenido que quieras, y nombre `f1.txt`, luego cargalo a tu entorno Jupyter a través del icono de upload &#x2B06;. Usaremos el mismo bucket creado en la actividad anterior\n",
    "\n",
    "Luego, **Usando el entorno de Jupyter**, ejecuta lo siguiente:\n",
    "\n",
    "- Copia el fichero en tu bucket, bajo el nombre `test/jupyter/file1`\n",
    "- Copia de nuevo el fichero, bajo el nombre `test/jupyter/file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/jupyter/`, deberían aparecer 2 ficheros\n",
    "- Borra el fichero `file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/jupyter/`, debería aparecer 1 fichero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Big data desde S3 \n",
    "\n",
    "En esta parte, intentaremos procesar la colección entera de Tweets que reside en S3. Los pasos a seguir serán los siguientes: \n",
    "\n",
    "1. Desde la consola del Learner Lab, copiar la colección entera desde el bucket de origen (`mudab-2025-big-data`) bajo el path `twitter-data`. Los ficheros se denominan `Eurovision-XX.json` donde `XX` es un número entre `00` y `09`. Para hacer una copia recursiva de ficheros se puede utilizar el comando de copia de una manera más sofisticada utilizando la opción `--recursive`. \n",
    "```bash\n",
    "aws s3 cp —recursive s3://mudab-2023/twitter-data/ s3://mudab2023-i123456/input/\n",
    "```\n",
    "\n",
    "2. Utilizar el mísmo código de la versión anterior para contestar a las siguientes preguntas: \n",
    "- Cuántos Tweeets son en español? \n",
    "- Cual son las 100 palabras más frecuentes en español? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5MGuGGFJy2-"
   },
   "source": [
    "## Modelo de Tweets\n",
    "\n",
    "Similar a la clase anterior, creamos un modelo de Tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF2lAty2c4yy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "  \"\"\"Class to model a Tweet\"\"\"\n",
    "  id: int         # The unique ID of a tweet\n",
    "  content: str    # The textual content of a tweet\n",
    "  author: str     # The nickname of the author of the tweet\n",
    "  language: str   # The language of the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de input (ETL) \n",
    "\n",
    "Similar a la clase anterior, almacenamos todos los tweets en memoria. \n",
    "\n",
    "**Pregunta**: funcionarà esto para nuestro input completo? Que hacer si así no es? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ghv9ixwmcbk9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, dataclasses\n",
    "\n",
    "tweets = []\n",
    "\n",
    "def parse_line(line: str):\n",
    "  \"\"\"Try to parse a string into a Person\"\"\"\n",
    "  try:\n",
    "    parsed = json.loads(line)\n",
    "    return Tweet(parsed['id'], parsed['text'], parsed['user']['name'], parsed['lang'])\n",
    "  except Exception as e:\n",
    "    print(f\"Error parsing '{line}': {e}\")\n",
    "\n",
    "with s3.open(data_location) as input:\n",
    "  for line in input.readlines():\n",
    "    if len(line.strip()) > 0:\n",
    "      tweet = parse_line(line)\n",
    "      if tweet: # We add only if the tweet is not 'None'\n",
    "         tweets.append(tweet)\n",
    "\n",
    "for modeled_tweet in tweets[0:10]:\n",
    "  print(modeled_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4PHW8O8wbCO"
   },
   "source": [
    "# Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0M228jcmOWBS",
    "outputId": "8976386a-0645-4be9-a1e0-56bc47767f69",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json, dataclasses\n",
    "\n",
    "def read_clean_tweets(input: str):\n",
    "  tweets = []\n",
    "  with open(input, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "  for line in lines:\n",
    "    parsed = json.loads(line)\n",
    "    tweet = Tweet(**parsed)\n",
    "    tweets.append(tweet)\n",
    "  return tweets\n",
    "\n",
    "def count_tweets(language: str, tweets: list[Tweet]):\n",
    "  count = 0\n",
    "  for tweet in tweets:\n",
    "    if (tweet.language == language):\n",
    "      count = count + 1\n",
    "  return count\n",
    "\n",
    "def most_frequent_word(tweets: list[Tweet]):\n",
    "  count = {}\n",
    "  for tweet in tweets:\n",
    "    words = tweet.content.split(' ')\n",
    "    for word in words:\n",
    "      if (word in count):\n",
    "        new_val = count[word] + 1\n",
    "        count[word] = new_val\n",
    "      else:\n",
    "        count[word] = 1\n",
    "  return dict(sorted(count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "spanish_tweets_count = count_tweets('es', tweets)\n",
    "\n",
    "words_by_frequence = list(most_frequent_word(tweets).items())[0:100]\n",
    "\n",
    "print(spanish_tweets_count)\n",
    "print(words_by_frequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwjHMWoRCGn7"
   },
   "source": [
    "# Question 3 (Home, Moodle)\n",
    "\n",
    "Los ejemplos de arriba se han hecho con *un solo* fichero, pero las siguientes preguntas aplican a **TODA** la colección (el conjunto de 10 ficheros). \n",
    "\n",
    "- 3.1. Cuántos tweets en español son originales (es decir, no son retweets)? \n",
    "- 3.2. Cual es el porcentaje de tweets para cada lenguaje? Es decir, de toda la colleccion, XX% son en idioma YY, ZZ% son en idioma WW, etc...\n",
    "- 3.3. Cuáles son las palabras más frecuentse en castellano? \n",
    "\n",
    "Añade tu respuesta en un bloque de Jupyter aquí abajo "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
