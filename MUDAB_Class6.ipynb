{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "081d9ab2-980d-42ce-bade-fd3fe5b12982",
   "metadata": {},
   "source": [
    "# DynamoDB \n",
    "\n",
    "## Crear una tabla para guardar los tweets\n",
    "\n",
    "El primer paso para insertar datos en DynamoDB es la creación de una tabla donde almacenar los datos. Esta operación Se puede hacer de las tres maneras de interactuar con AWS, pues en esta práctica lo haremos de forma programatica utilizando el SDK de python: [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a1b070-656a-4ac2-b74e-6114bd6dc9b1",
   "metadata": {},
   "source": [
    "### Crear un cliente para DynamoDB\n",
    "\n",
    "Las siguientes líneas crean un client para DynamoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b62f7f94-b45d-4776-92f0-32317286d5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize the DynamoDB resource\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=\"us-east-1\")\n",
    "dynamodb_client = boto3.client(\"dynamodb\", region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2abbcf-b80c-4d29-a021-e2efb53df8b1",
   "metadata": {},
   "source": [
    "`import boto3`: Imports the Boto3 library, which is the Amazon Web Services (AWS) SDK for Python. We can use services like Amazon S3 and Amazon EC2.\n",
    "\n",
    "`boto3.resource()`: Provides a higher-level abstraction and is typically used for interacting with DynamoDB tables in an object-oriented way. It's often used for simple CRUD (Create, Read, Update, Delete) operations.\n",
    "\n",
    "`boto3.client()`: Provides a lower-level abstraction and is typically used for more complex operations, such as batch operations or when you need more fine-grained control over the requests being sent to DynamoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fb99d-c7c9-4ecb-904c-ce6fbeef6d85",
   "metadata": {},
   "source": [
    "## Crear tablas para DynamoDB\n",
    "\n",
    "Cada tabla de DynamoDB tiene que proporcionar un nombre y un esquema para las claves de la BB.DD (*Partition Key*, y *Hash Key* si es necesaria). La sintaxis del método `create_table` necesita varios parámetros: \n",
    "- el `nombre de la tabla`\n",
    "- el `esquema para la claves`\n",
    "- el `tipo de claves`\n",
    "- el `throughput esperado para esta tabla`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519b854d-4540-4ca4-8b9d-eef30f0dcf01",
   "metadata": {},
   "source": [
    "### Crear una tabla para tweets per idioma y timestamp \n",
    "\n",
    "Usaríamos este código si crearamos las tablas manualmente. Pero en nuestro caso haremos importación desde S3, para cargar un gran volument de datos, así que este código nos sirve solo como referencia. Podemos probarlo, pero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95a7125e-a049-445d-8c5a-f37b16d41e13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'TestTable' created successfully!\n"
     ]
    }
   ],
   "source": [
    "table_name = \"TestTable\" \n",
    "table = dynamodb.create_table(\n",
    "    TableName=table_name,\n",
    "    # KEY SCHEMA\n",
    "    KeySchema=[\n",
    "        {\"AttributeName\": \"user\", \"KeyType\": \"HASH\"},  # Partition Key (HASH)\n",
    "        {\"AttributeName\": \"timestamp\", \"KeyType\": \"RANGE\"},  # Sort Key (RANGE)\n",
    "    ], \n",
    "    # DEFINE SCHEMA\n",
    "    AttributeDefinitions=[\n",
    "        {\"AttributeName\": \"user\", \"AttributeType\": \"S\"}, # USER IS A STRING\n",
    "        {\"AttributeName\": \"timestamp\", \"AttributeType\": \"N\"} # TIMESTAMP IS A NUMBER \n",
    "    ], # AN ATTRIBUTE OF AN ELEMENT CAN BE A LIST\n",
    "    # WE CAN TO WRITE 5 UNITS AND READ 5. RELATED TO PROVISIONED CAPACITY\n",
    "    ProvisionedThroughput={\"ReadCapacityUnits\": 5, \"WriteCapacityUnits\": 5},\n",
    ")\n",
    "\n",
    "# Wait for the table to be created\n",
    "table.meta.client.get_waiter(\"table_exists\").wait(TableName=table_name)\n",
    "print(f\"Table '{table_name}' created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddb3b7af-4640-4743-914a-c8a1b934d718",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TableDescription': {'TableName': 'TestTable',\n",
       "  'TableStatus': 'DELETING',\n",
       "  'ProvisionedThroughput': {'NumberOfDecreasesToday': 0,\n",
       "   'ReadCapacityUnits': 5,\n",
       "   'WriteCapacityUnits': 5},\n",
       "  'TableSizeBytes': 0,\n",
       "  'ItemCount': 0,\n",
       "  'TableArn': 'arn:aws:dynamodb:us-east-1:842912048264:table/TestTable',\n",
       "  'TableId': '2ed1f792-0665-4fc7-a298-f07dd5bde7c7',\n",
       "  'DeletionProtectionEnabled': False},\n",
       " 'ResponseMetadata': {'RequestId': 'UA8HSRI9PJR00MMBRTJI7KKPTRVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'server': 'Server',\n",
       "   'date': 'Wed, 12 Feb 2025 18:53:08 GMT',\n",
       "   'content-type': 'application/x-amz-json-1.0',\n",
       "   'content-length': '354',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': 'UA8HSRI9PJR00MMBRTJI7KKPTRVV4KQNSO5AEMVJF66Q9ASUAAJG',\n",
       "   'x-amz-crc32': '2975200431'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Delete the table\n",
    "dynamodb_client.delete_table(TableName=table_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3a52b-2da6-46ec-85f5-820001bd9e2c",
   "metadata": {},
   "source": [
    "## Modelizar tweets\n",
    "\n",
    "Vamos a enriqueces el model de Tweet que ya conocemos extrayendo también el `timestamp` (el tiempo de creación exacto del tweet). \n",
    "\n",
    "Añadimos también un método (`to_dynamo_json`) que devuelve el mismo Tweet como `Item` de DynamoDB en formato JSON: esto significa que Dynamo puede leer JSON (en un formato especifico para Dynamo) y \"cargarlos\" en la BB.DD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09584f0b-4ca7-456d-a476-6f8dbca1fd4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import json \n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    \"\"\"Class to model a Tweet\"\"\"\n",
    "    id: int         # The unique ID of a tweet\n",
    "    content: str    # The textual content of a tweet\n",
    "    author: str     # The nickname of the author of the tweet\n",
    "    language: str   # The language of the tweet\n",
    "    timestamp: int  # The timestamp of the tweet, in epoch\n",
    "\n",
    "    def to_dynamo_json_by_lang(self):\n",
    "        tweet = {\n",
    "            \"Item\": {\n",
    "                \"language\": {\n",
    "                    \"S\": self.language\n",
    "                },\n",
    "                \"timestamp_tweetid\": {\n",
    "                    \"S\": f'{self.timestamp}#{self.id}'\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"S\": self.content\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return json.dumps(tweet)\n",
    "    \n",
    "    def to_dynamo_json_by_user(self):\n",
    "        tweet = {\n",
    "            \"Item\": {\n",
    "                \"user\": {\n",
    "                    \"S\": self.author\n",
    "                },\n",
    "                \"timestamp\": {\n",
    "                    \"N\": f\"{self.timestamp}\"\n",
    "                },\n",
    "                \"content\": {\n",
    "                    \"S\": self.content\n",
    "                },\n",
    "                \"language\": {\n",
    "                    \"S\": self.language\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return json.dumps(tweet)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6fd41-845b-4a7e-943a-e6ec4e9d581e",
   "metadata": {},
   "source": [
    "This script defines a Python class named Tweet using the @dataclass decorator. The Tweet class is designed to model a tweet with its associated attributes.\n",
    "\n",
    "Attributes:\n",
    "- id: a unique integer identifier for the tweet\n",
    "- content: the textual content of the tweet as a string\n",
    "- author: the nickname of the author of the tweet as a string\n",
    "- language: the language of the tweet as a string\n",
    "- timestamp: the timestamp of the tweet in epoch time (the number of seconds that have elapsed since January 1, 1970) as an integer\n",
    "\n",
    "Methods:\n",
    "The Tweet class has two methods:\n",
    "- `to_dynamo_json_by_lang`: This method returns a JSON string representing the tweet in a format suitable for Amazon DynamoDB. The primary key is a composite key consisting of the language and a concatenation of the timestamp and tweet ID.\n",
    "- `to_dynamo_json_by_user`: This method returns a JSON string representing the tweet in a format suitable for Amazon DynamoDB. The primary key is the author (user), and the sort key is the timestamp.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74d2062-f868-4213-85e1-6ebe4a5e0f3c",
   "metadata": {},
   "source": [
    "### Ejemplo de Tweet y de su formato JSON para DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f42682a-9689-4bae-92cb-b0a8df0c3c0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"Item\": {\"language\": {\"S\": \"es\"}, \"timestamp_tweetid\": {\"S\": \"1234567890#123\"}, \"content\": {\"S\": \"esto es el contenido de un Tweet\"}}}\n",
      "{\"Item\": {\"user\": {\"S\": \"luca\"}, \"timestamp\": {\"N\": \"1234567890\"}, \"content\": {\"S\": \"esto es el contenido de un Tweet\"}, \"language\": {\"S\": \"es\"}}}\n"
     ]
    }
   ],
   "source": [
    "tweet = Tweet(123, \"esto es el contenido de un Tweet\", \"luca\", \"es\", 1234567890)\n",
    "print(tweet.to_dynamo_json_by_lang())\n",
    "print(tweet.to_dynamo_json_by_user())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e28864-46b2-413e-97cb-df7d10fbe2b0",
   "metadata": {},
   "source": [
    "## Cargar tweets a Dynamo DB\n",
    "\n",
    "No podemos insertar un Tweet a la vez en Dynamo: insertar 1M de tweets tardaría horas! Esto porque Dynamo no está hecho para procesar un enorme volumen de tráfico en un tiempo limitado (*batch*), sino - similarmente a una BB.DD. convencional - un tráfico constante a lo largo del tiempo (*real-time*). \n",
    "\n",
    "Por lo tanto usaremos una solución alternativa. Para poder cargar un volumen de datos interesante, utilizaremos la opción de cargar una tabla desde S3. Esto se traduce en que, utilizando Spark, leeremos unos datos de input y los transformaremos en unos datos de output que almacenaremos en S3 y que se podrán cargar en DynamoDB, respetando el formato de representación de datos de entrada de DynamoDB. \n",
    "\n",
    "El proceso será: \n",
    "\n",
    "- (Notebook) Usar spark para leer datos de input, y transformalos en datos de output aptos para la carga. Tendremos que escribir estos datos en S3. \n",
    "- (Consola Web de AWS) Usar la funcionalidad de Dynamo de cargar desde S3 para indicar nuestro directorio de output como fuente de datos para una tabla. \n",
    "\n",
    "Este proceso se tendrá que **realizar una sola vez**, y será nuestra manera de *simular un volumen de datos entrantes* en DynamoDB\n",
    "\n",
    "Para poder alcanzar este objectivo con éxito, necesitaremos superar un escollo con que hasta la fecha no hemos trabajado: **las credenciales de AWS**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a946ec-68bf-4388-809d-0ee828d89e58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/12 19:05:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import os \n",
    "\n",
    "# os.environ['AWS_PROFILE'] = \"default\"\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars()))\n",
    "# USE OUR BUCKET AND WRITE ON OUR OWN BUCKET\n",
    "conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"com.amazonaws.auth.DefaultAWSCredentialsProviderChain\")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    .appName(\"test\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d6bc65-f697-4b3a-bc59-7ca699d3b2de",
   "metadata": {},
   "source": [
    "Set up a SparkSession with AWS credentials and SageMaker PySpark configuration. \n",
    "Importing Libraries\n",
    "The script starts by importing the necessary libraries:\n",
    "- `pyspark`: the Python API for Apache Spark\n",
    "- `sagemaker_pyspark`: a library that integrates SageMaker with PySpark\n",
    "- `os`: a library for interacting with the operating system\n",
    "\n",
    "Setting AWS Credentials\n",
    "The script sets the AWS_PROFILE environment variable to \"default\". This tells the AWS SDK to use the default credentials profile, which is typically stored in the `~/.aws/credentials` file.\n",
    "\n",
    "Configuring Spark\n",
    "- The script creates a `SparkConf` object and sets two configuration properties:\n",
    "\n",
    "-   `spark.driver.extraClassPath`: this property is set to the list of JAR files required by SageMaker PySpark, which are obtained using sagemaker_pyspark.classpath_jars(). The \":\".join() method is used to concatenate the list of JAR files into a single string, separated by colons.\n",
    "\n",
    "-   `spark.hadoop.fs.s3a.aws.credentials.provider`: this property is set to com.amazonaws.auth.DefaultAWSCredentialsProviderChain, which tells Spark to use the default AWS credentials provider chain to authenticate with S3.\n",
    "\n",
    "Creating a SparkSession\n",
    "The script creates a SparkSession using the SparkSession.builder API. The config method is used to pass the SparkConf object created earlier, and the appName method is used to set the name of the Spark application to \"test\". Finally, the getOrCreate method is used to create the SparkSession if it doesn't already exist.\n",
    "\n",
    "Getting the SparkContext\n",
    "The script gets the `SparkContext` object from the `SparkSession` using the `sparkContext` attribute. The `SparkContext` is the core entry point for `Spark functionality`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06a69b65-55a8-4ba9-9676-615cdc90ffa2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tweet(id=995443356309311493, content='RT @jk_rowling: France ❤️ #Eurovision', author='_hoodstery', language='fr', timestamp=1526167165244)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def toTweet(line: str):\n",
    "      try:\n",
    "        parsed = json.loads(line)\n",
    "        epoch = int(parsed['timestamp_ms'])\n",
    "        ## NOTA: usando ['user']['screen_name']\n",
    "        return Tweet(parsed['id'], parsed['text'], parsed['user']['screen_name'], parsed['lang'], epoch)\n",
    "      except Exception as e:\n",
    "        return None\n",
    "    \n",
    "# ONLY READING 1 FILE\n",
    "rdd = sc.textFile('s3a://mudab-2025-big-data/twitter-data/Eurovision-00.json')\n",
    "processed = (rdd  # rdd[string]\n",
    "    .map(toTweet) # parse string into Tweet. Return rdd[Tweet]\n",
    "    .filter(lambda x: x is not None) # filter empty values. Return same type (rdd[Tweet])\n",
    ")\n",
    "rdd.take(1)\n",
    "processed.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9bcbe78-313c-4770-b7ce-38cb59e6985f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ELEMENTS TO INSERT INTO A TABLE IN DYNOMO BD\n",
    "dynamo_by_lang = processed.map(lambda x: x.to_dynamo_json_by_lang()) # RDD[TWEET].  \n",
    "dynamo_by_user = processed.map(lambda x: x.to_dynamo_json_by_user()) # RDD[]\n",
    "\n",
    "dynamo_by_lang.saveAsTextFile('s3a://mudab-2025-pc1262057/twitter-data-for-dynamo/by_lang/1/')\n",
    "dynamo_by_user.saveAsTextFile('s3a://mudab-2025-pc1262057/twitter-data-for-dynamo/by_user/1/') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ec14a-9aa5-4a80-8564-4b238d70f9bd",
   "metadata": {},
   "source": [
    "The resulting RDDs, `dynamo_by_lang` and `dynamo_by_user`, contain JSON strings that can be written to Amazon DynamoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28489d10-598b-44c8-9820-aed0444d4559",
   "metadata": {},
   "source": [
    "El siguiente calculo proporciona los timestamp a los extremos de nuestra colección. Son **valores en milisegundos**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "693f5665-a42a-4322-8529-4c3b9cdd173d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:======================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower ts: 1526167165244, higher ts: 1526169318161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "max_ts = processed.map(lambda x: x.timestamp).reduce(lambda x, y: max(x, y))\n",
    "min_ts = processed.map(lambda x: x.timestamp).reduce(lambda x, y: min(x, y))\n",
    "\n",
    "print(f'Lower ts: {min_ts}, higher ts: {max_ts}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e6f701-6c7f-49dc-b770-d2041962fe66",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cargar datos en Dynamo\n",
    "\n",
    "Una vez procesados los Tweets, los podemos cargar en Dynamo DB de manera batch a través de la consola de S3. Este proceso tardará unos minutos, y lo realizaremos a través de la interfaz Web de amazon "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4d085-88ee-4c87-9c87-af5927c0c809",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Query Dynamo \n",
    "\n",
    "Antes de comenzar, tenemos que tener dos tablas después de cargar datos via S3 \n",
    "\n",
    "### Tabla de Usuarios por Tiempo \n",
    "\n",
    "Tiene el siguiente aspecto: \n",
    "- la primary key es el nombre de usuario: 'username'\n",
    "- la sort key es el `timestamp` del Tweet\n",
    "\n",
    "### Tabla de Idiomas por Tiempo \n",
    "\n",
    "Tiene el siguiente aspecto: \n",
    "- la primary key es el idioma: 'language'\n",
    "- la sort key es el `timestamp + el ID` del Tweet: dos campos se suelen separar por el caracter `#`, de forma que un valor aparecerà come `123#321`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0cf070c-a51a-4f85-b02e-3be36603cad6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tweet_by_user_table = \"TweetByUser\"\n",
    "tweet_by_language_table = \"TweetByLang\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d103587-776a-4371-be1f-6734f3a28743",
   "metadata": {},
   "source": [
    "### Últimos tweets por usuario \n",
    "\n",
    "Un caso de uso típico para DynamoDb podría ser por ejemplo de visualizar los últimos N tweets de un usuario especifico. En este caso, con el ID de usuario como *Partition Key*, y el timestamp como *Sort Key* será muy sencillo realizar una query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbde39f9-7f57-454d-9a40-375880748ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'RT @IgualdadLGBT: Tenemos un nuevo referente lésbico, Saara Aalto, cantante de Finlandia #Eurovision #FinalEurovision https://t.co/zEDD3dUj…', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169312928')}\n",
      "{'content': 'RT @playz: #Eurovision #FinalEurovision ¡Se acabó! Así ha terminado la jornada de @Ricky_ot2017 en el Instagram de Playz. Ahora, ¡todos de…', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169255068')}\n",
      "{'content': 'RT @ManelNMusic: Oye Israel, que esto lo hice yo antes que tu... #Eurovision #FinalEurovision https://t.co/bBnDjB9s4z', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169228713')}\n",
      "{'content': \"RT @eurovision_tve: 🔴 CRÓNICA Israel y el baile de la 'gallina' ganan #Eurovision en el año en el que Amaia y Alfred enamoraron a España ht…\", 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169225734')}\n",
      "{'content': 'RT @VodafoneTV_es: Israel ha jugado con fuego pero consigue llevarse #Eurovision ¡¡Enhorabuena!! 👏👏👏#TodoEstáPorVer https://t.co/p2puU6wUKG', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169222268')}\n",
      "{'content': 'RT @eurovision_tve: Y esta es la actuación con la que Netta Barzilai ha ganado #eurovision https://t.co/dI0104t9k8 #FinalEurovision https:/…', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169219097')}\n",
      "{'content': 'RT @FormulaTV: RT si te gustaría ver esto en Eurovisión 2019 #Eurovision #FinalEurovision https://t.co/9aTw2iQarA https://t.co/4qb9h0HgQ5', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169217138')}\n",
      "{'content': 'RT @eurovision_tve: ¡¡Se desata la locura en el Altice Arena!! Netta se corona ganadora de #Eurovision #FinalEurovision https://t.co/27h0Vf…', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169214205')}\n",
      "{'content': 'RT @eurovision_tve: ¡Muy atentos porque en nada vamos a hablar con Alfred y Amaia! #Eurovision #FinalEurovision https://t.co/y7iCLCJ5df htt…', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169211953')}\n",
      "{'content': 'RT @eurovision_tve: No habrá ganado, pero @CesarSampson_ se lleva este #parecidorazonable con @pabloalboran 🤣 🤣  #Eurovision #FinalEurovisi…', 'user': 'celia22975', 'language': 'es', 'timestamp': Decimal('1526169208006')}\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from boto3.dynamodb.conditions import Key\n",
    "\n",
    "table = dynamodb.Table(tweet_by_user_table)\n",
    "\n",
    "def last_10_tweets_for_user(user: str): \n",
    "    # Query for the last 10 tweets for a specific user\n",
    "    response = table.query(\n",
    "        KeyConditionExpression=Key('user').eq(user), # OUR USER MUST BE THE SAME AS THE VALUE WE PASSED\n",
    "        ScanIndexForward=False,  # Descending order: latest tweets first\n",
    "        Limit=10\n",
    "    )\n",
    "    # Process the response\n",
    "    tweets = response.get('Items', [])\n",
    "    return tweets\n",
    "\n",
    "for tweet in last_10_tweets_for_user('celia22975'):\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a97488-20da-4803-9f2f-13564c730a83",
   "metadata": {},
   "source": [
    "#### Query para todos los tweets en un idioma dentro de un interval de tiempo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d701454e-9737-441a-8dc3-808d59888335",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865\n",
      "{'content': 'RT @britneyxcheetos: you mean Chub-Bi https://t.co/KijWpZxRYR', 'language': 'en', 'timestamp_tweetid': '1526167165271#995443356422557696'}\n",
      "{'content': 'RT @snugglycamila: i’m not surprised eurovision is confusing for americans since the concept of the person with the most votes actually win…', 'language': 'en', 'timestamp_tweetid': '1526167165367#995443356825202690'}\n",
      "{'content': 'RT @gi_de_off: #Eurovision \\ni just adopted two boys 🇺🇦❤️🇨🇿 https://t.co/2ZSvphRPxV', 'language': 'en', 'timestamp_tweetid': '1526167165419#995443357043298304'}\n",
      "{'content': 'hey #EUROVISION next year i am thinking of participating in eurovision song contest as spongebob squerapants... wha… https://t.co/PR24YGk0Hk', 'language': 'en', 'timestamp_tweetid': '1526167165466#995443357240393728'}\n",
      "{'content': 'RT @davidmackau: my fav #eurovision tradition is the international BuzzFeed accounts coming together to roast the shit out of @BuzzFeedUK h…', 'language': 'en', 'timestamp_tweetid': '1526167165502#995443357391425536'}\n",
      "{'content': \"RT @patronusblake: honestly i feel bad for the people that don't have graham norton him roasting the contestants is the funniest thing #Eur…\", 'language': 'en', 'timestamp_tweetid': '1526167165541#995443357555019777'}\n",
      "{'content': 'RT @bbceurovision: The moment the Israeli delegation found out they had won Eurovision 2018 🎉🐔🎈👏👏👏 Congratulations @NettaBarzilai #Eurovisi…', 'language': 'en', 'timestamp_tweetid': '1526167165596#995443357785772033'}\n",
      "{'content': \"RT @BesaXani: We had Albania Italy Germany Austria Ireland Lithuania UK.. with real music and meaningful lyrics but y'all had to give the f…\", 'language': 'en', 'timestamp_tweetid': '1526167165633#995443357940895745'}\n",
      "{'content': 'RT @lauralovestaylo: SALVADOR SOBRAL IS ONE OF US #Eurovision https://t.co/D2ySAfHYi4', 'language': 'en', 'timestamp_tweetid': '1526167165639#995443357966102529'}\n",
      "{'content': 'Wait people actually watch Eurovision? Im confused', 'language': 'en', 'timestamp_tweetid': '1526167165654#995443358028976128'}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime \n",
    "def get_tweets_in_time_range(language, start_time, end_time):\n",
    "    \"\"\"Fetches tweets within a specific time range.\"\"\"\n",
    "    table = dynamodb.Table(tweet_by_language_table)\n",
    "\n",
    "    start_key = f\"{start_time}#\"\n",
    "    end_key = f\"{end_time}#\"\n",
    "\n",
    "    response = table.query(\n",
    "        KeyConditionExpression=\"#lang = :val AND timestamp_tweetid BETWEEN :start AND :end\",\n",
    "        ExpressionAttributeNames={\"#lang\": \"language\"},  # Alias for reserved keyword\n",
    "        ExpressionAttributeValues={\":val\": language, \":start\": start_key, \":end\": end_key},\n",
    "        ScanIndexForward=True,  # True = Oldest first\n",
    "    )\n",
    "    return response.get(\"Items\", [])\n",
    "\n",
    "start = min_ts\n",
    "recent_tweets = get_tweets_in_time_range(\"en\", start, start + 60000)\n",
    "print(len(recent_tweets))\n",
    "for tweet in recent_tweets[0:10]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f644233e-d757-423d-88b0-8820238ce8f9",
   "metadata": {},
   "source": [
    "This script is designed to fetch tweets within a specific time range from a DynamoDB table. Here's a breakdown of how it works:\n",
    "The script starts by importing the datetime module, which is not used in this snippet, but might be used elsewhere in the code to handle date and time operations.\n",
    "\n",
    "The get_tweets_in_time_range function takes three parameters:\n",
    "- language: the language of the tweets to be fetched\n",
    "- start_time: the start of the time range\n",
    "- end_time: the end of the time range\n",
    "\n",
    "The function uses the dynamodb library to interact with a DynamoDB table. The table name is stored in the tweet_by_language_table variable, which is not defined in this snippet.\n",
    "The start_key and end_key variables are created by concatenating the start_time and end_time with a \"#\" symbol. This is likely done to create a unique key for the DynamoDB query.\n",
    "\n",
    "The table.query method is used to fetch tweets from the DynamoDB table. The query uses the following parameters:\n",
    "KeyConditionExpression: specifies the conditions for the query. In this case, it's looking for tweets where the language attribute matches the :val parameter, and the timestamp_tweetid attribute is between the :start and :end parameters.\n",
    "\n",
    "ExpressionAttributeNames: provides an alias for the language attribute, which is a reserved keyword in DynamoDB.\n",
    "\n",
    "ExpressionAttributeValues: provides the values for the :val, :start, and :end parameters.\n",
    "ScanIndexForward: specifies the order in which the results are returned. In this case, it's set to True, which means the results are returned in ascending order (i.e., oldest tweets first).\n",
    "\n",
    "The function returns a list of tweets that match the query conditions. If no tweets are found, an empty list is returned.\n",
    "\n",
    "The script then calls the get_tweets_in_time_range function with the following parameters:\n",
    "- language: \"en\" (English)\n",
    "- start_time: min_ts (which is not defined in this snippet)\n",
    "- end_time: min_ts + 60000 (which is 1 minute after min_ts)\n",
    "\n",
    "The script prints the number of tweets returned by the function, and then prints the first 10 tweets in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3452376c-3d40-4af1-bc55-a3c8ca09cfee",
   "metadata": {},
   "source": [
    "### Serie temporal por idioma \n",
    "\n",
    "Podemos crear una serie temporal para visualizar el número de tweets por idioma en intervalos consecutivos de tiempo. Para obtener el dato bruto, podemos hacer una secuencia de queries a Dynamo. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4910a8e-aee7-43b4-826c-07ffd759b753",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2018-05-12 23:19:00', 488), ('2018-05-12 23:20:00', 884), ('2018-05-12 23:21:00', 880), ('2018-05-12 23:22:00', 912), ('2018-05-12 23:23:00', 886), ('2018-05-12 23:24:00', 887), ('2018-05-12 23:25:00', 924), ('2018-05-12 23:26:00', 876), ('2018-05-12 23:27:00', 877), ('2018-05-12 23:28:00', 887), ('2018-05-12 23:29:00', 849), ('2018-05-12 23:30:00', 872), ('2018-05-12 23:31:00', 880), ('2018-05-12 23:32:00', 840), ('2018-05-12 23:33:00', 842), ('2018-05-12 23:34:00', 880), ('2018-05-12 23:35:00', 846), ('2018-05-12 23:36:00', 866), ('2018-05-12 23:37:00', 862), ('2018-05-12 23:38:00', 852), ('2018-05-12 23:39:00', 845), ('2018-05-12 23:40:00', 764), ('2018-05-12 23:41:00', 809), ('2018-05-12 23:42:00', 841), ('2018-05-12 23:43:00', 855), ('2018-05-12 23:44:00', 832), ('2018-05-12 23:45:00', 850), ('2018-05-12 23:46:00', 832), ('2018-05-12 23:47:00', 881), ('2018-05-12 23:48:00', 800), ('2018-05-12 23:49:00', 829), ('2018-05-12 23:50:00', 818), ('2018-05-12 23:51:00', 817), ('2018-05-12 23:52:00', 838), ('2018-05-12 23:53:00', 856), ('2018-05-12 23:54:00', 837), ('2018-05-12 23:55:00', 267)]\n",
      "[('2018-05-12 23:19:00', 5763), ('2018-05-12 23:24:00', 5779), ('2018-05-12 23:29:00', 5769), ('2018-05-12 23:34:00', 5786), ('2018-05-12 23:39:00', 5782), ('2018-05-12 23:44:00', 5825), ('2018-05-12 23:49:00', 5820), ('2018-05-12 23:54:00', 1894)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from boto3.dynamodb.conditions import Key\n",
    "import boto3\n",
    "\n",
    "# Initialize the DynamoDB resource and table\n",
    "dynamodb = boto3.resource(\"dynamodb\", region_name=\"us-east-1\")\n",
    "table = dynamodb.Table(tweet_by_language_table)\n",
    "\n",
    "def count_tweets_for_language_in_interval(language, start_ts, end_ts):\n",
    "    \"\"\"\n",
    "    Count tweets for a given language between start_ts and end_ts.\n",
    "    \"\"\"\n",
    "    # Construct the lower and upper bounds of the sort key.\n",
    "    # Note: This assumes that the sort key is stored as \"timestamp#tweet_id\"\n",
    "    start_key = f\"{start_ts}#\"\n",
    "    end_key = f\"{end_ts}#\"    \n",
    "    response = table.query(\n",
    "        KeyConditionExpression=Key('language').eq(language) & \n",
    "                               Key('timestamp_tweetid').between(start_key, end_key),\n",
    "        Select='COUNT'\n",
    "    )\n",
    "    return response.get('Count', 0)\n",
    "\n",
    "def to_readable_date(timestamp_ms: str): \n",
    "    utc_date = datetime.utcfromtimestamp(timestamp_ms/1000)\n",
    "    formatted_utc = utc_date.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    return formatted_utc\n",
    "\n",
    "def timeseries(lang: str, time_from: int, time_to: int, delta: int):\n",
    "    \"\"\"\n",
    "    Count tweets in a specific language and interval, with a fixed step.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    current_ts = time_from\n",
    "    while(current_ts < time_to):\n",
    "        tweet_count = count_tweets_for_language_in_interval(lang, current_ts, current_ts + delta)\n",
    "        values.append((to_readable_date(current_ts), tweet_count))\n",
    "        current_ts += delta\n",
    "    return values\n",
    "\n",
    "start_time = min_ts - (min_ts % 60000)\n",
    "one_minute = 60 * 1000\n",
    "\n",
    "en_ts_1m = timeseries('en', start_time, max_ts, one_minute)\n",
    "es_ts_5m = timeseries('es', start_time, max_ts, 5 * one_minute)\n",
    "\n",
    "\n",
    "print(en_ts_1m)\n",
    "print(es_ts_5m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
