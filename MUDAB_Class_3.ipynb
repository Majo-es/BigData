{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7e3SpPNdP8As"
   },
   "source": [
    "# Class 3 - Usar ficheros desde S3\n",
    "\n",
    "## Objetivo\n",
    "\n",
    "Usar datos reales de gran tama√±o almacenados en S3\n",
    "\n",
    "### Steps\n",
    "\n",
    "1. subir datos a S3\n",
    "2. utilizar S3 como fuente de datos de Tweets \n",
    "3. contestar preguntas sobre estos datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1: Operaciones en S3\n",
    "\n",
    "Para praticar las operaciones disponibles en S3 usaremos la consola del Learner Lab. \n",
    "\n",
    "## Buckets \n",
    "\n",
    "### Crear un bucket \n",
    "\n",
    "Para crear el bucket `bucket-de-ejemplo`, el comando es: \n",
    "\n",
    "```aws\n",
    "aws s3api create-bucket -- bucket bucket-de-ejemplo\n",
    "```\n",
    "\n",
    "Con el comando \n",
    "```aws\n",
    "aws s3api list-buckets  \n",
    "```\n",
    "\n",
    "podemos comprobar que el bucket haya sido creado. \n",
    "\n",
    "&#x26A0; **Importante!** No pueden existir globalmente en S3 dos buckets con el mismo nombre, aunque sean creados por personas/cuentas diferentes. \n",
    "\n",
    "### Eliminar un bucket \n",
    "\n",
    "Para eliminar el bucket `bucket-de-ejemplo`, el comando es: \n",
    "\n",
    "```aws\n",
    "aws s3api delete-bucket -bucket bucket-de-ejemplo\n",
    "```\n",
    "\n",
    "Con el comando \n",
    "```aws\n",
    "aws s3api list-buckets  \n",
    "```\n",
    "\n",
    "podemos comprobar que el bucket haya sido eliminado. \n",
    "\n",
    "## Ficheros\n",
    "\n",
    "### Hacer el listado de ficheros en un directorio \n",
    "\n",
    "En S3 los directorios como si no existen, pero cuando dos objetos tienen un prefijo comun separado por `/` este prefijo comun se puede interpretar como un directorio. Por ejemplo, el fichero con key `test/sub/file1` y el fichero con key `test/sub/file2` comparten el mismo prefijo `test/sub`, que podemos interpretar como que en el bucket contiene un directorio `test` que contiene un directorio `sub`, que contiene dos ficheros `file1` y `file2`. \n",
    "\n",
    "Por lo tanto, imaginando que estos ficheros sean dentro de un bucket llamado `my-bucket`, podemos pedir por consola de ver el contenido del directorio `test/sub` con el siguiente comando:\n",
    "\n",
    "```\n",
    "aws s3 ls s3://my-bucket/test/sub \n",
    "```\n",
    "\n",
    "### Copiar un fichero a S3 \n",
    "\n",
    "Un fichero `my-file.txt` desde el ordenador se puede copiar a S3 con el siguiente comando:\n",
    "\n",
    "```\n",
    "aws s3 cp my-file.txt s3://my-bucket/my-dir/my-file.txt\n",
    "```\n",
    "\n",
    "Tambi√©n es posible copiar un fichero de S3 a S3, siempre y cuando tengamos los permisos suficientes para realizar la operaci√≥n (permisos de lectura del fichero de origen, y permiso de escritura al bucket de destino).\n",
    "\n",
    "Por ejemplo el siguiente comando: \n",
    "```\n",
    "aws s3 cp s3://my-bucket/file1.txt s3://my-other-bucket/copy/file1\n",
    "```\n",
    "\n",
    "copia el fichero `file1.txt` desde el bucket `my-bucket` al bucket `my-other-bucket` con el key `copy/file1`. Esto quiere decir que el fichero `file1` estar√° autom√°ticamente en un directorio `copy` que se crear√° sin necesidad de existir previamente. Con el comando \n",
    "\n",
    "```\n",
    "aws s3 ls s3://my-other-bucket/copy/ \n",
    "```\n",
    "confirmaremos que el fichero est√° en el directorio `copy` bajo el nombre `file1` \n",
    "\n",
    "### Otras operaciones sobre ficheros \"locales\"\n",
    "\n",
    "Para hacer pruebas, puede ser √∫til crear ficheros **desde la consola del Learner Lab**. \n",
    "\n",
    "El siguiente comando crea un fichero vac√≠o llamado `my-file.txt`\n",
    "\n",
    "```bash\n",
    "touch my-file.txt\n",
    "```\n",
    "\n",
    "El siguiente comando imprime en pantalla el listado de ficheros en el directorio actual: \n",
    "\n",
    "```bash\n",
    "ls\n",
    "```\n",
    "\n",
    "Tambi√©n podemos crear un fichero con un contenido desde la consola: \n",
    "\n",
    "```bash\n",
    "echo '12345' > my-second-file.txt \n",
    "```\n",
    "\n",
    "El contenido del fichero `my-second-file.txt` ser√° el texto `12345`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actividad 1 (en clase)\n",
    "**Usando la consola del Learner Lab**, ejecuta lo siguiente:\n",
    "\n",
    "- Crea un bucket llamado `mudab-ixxxxx` donde `ixxxxx` es tu n√∫mero identificativo de estudiante\n",
    "- Crea un fichero local con el contenido `abcde` y nombre `file1.txt`\n",
    "- Copia el fichero en tu bucket, bajo el nombre `test/consola/file1`\n",
    "- Copia de nuevo el fichero, bajo el nombre `test/consola/file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/consola/`, deber√≠an aparecer 2 ficheros\n",
    "- Borra el fichero `file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/consola/`, deber√≠a aparecer 1 fichero\n",
    "\n",
    "### Descarga en tu ordenador  \n",
    "Si quieres tener los ficheros en tu ordenador para hacer pruebas por tu cuenta, pues los objetos p√∫blicos de S3 tiene un URL desde donde descargar el objeto. La versi√≥n comprimida de los tweet est√° disponible aqu√≠: https://mudab-2025-big-data.s3.us-east-1.amazonaws.com/twitter-data-compressed/twitter-data-from-eurovision-2018-splits.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nMLMr9Ks5_D"
   },
   "source": [
    "# Parte 2: S3 de manera program√°tica \n",
    "\n",
    "En esta parte, a√±adiremos el acceso a S3 como sistema de ficheros a trav√©s de una librer√≠a llamada [s3fs](https://s3fs.readthedocs.io/en/latest/). Esta librer√≠a nos permite acceder a S3 como si fuera un sistema de ficheros local. \n",
    "\n",
    "Aunque el acceso a S3 de manera programatica requiera credenciales, la ejecuci√≥n en entorno SageMaker nos permite omitir este paso de configuraci√≥n, y tener un c√≥digo m√°s sencillo. \n",
    "\n",
    "1. importo library s3\n",
    "2. create client de s3\n",
    "3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False) # CREATE CLIENT\n",
    "\n",
    "bucket='mudab-2025-big-data'\n",
    "data_key = 'twitter-data/Eurovision-00.json'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operaciones sobre ficheros con la librer√≠a s3fs \n",
    "\n",
    "La librer√≠a s3fs es un ejemplo de acceso program√°tico a los servicios de AWS. En este caso, nos permite acceder a S3 y hacer operaciones utilizando Python. Las operaciones sobre ficheros explicadas arriba se traducen en peque√±os *snippets* de c√≥digo.\n",
    "\n",
    "#### Hacer el listado de ficheros en un directorio\n",
    "\n",
    "```python\n",
    "s3.ls(\"my-bucket/test/sub\")\n",
    "```\n",
    "\n",
    "#### Copiar un fichero a S3\n",
    "\n",
    "```python\n",
    "s3.put(\"my-file.txt\", \"s3://my-bucket/my-dir/my-file.txt\")\n",
    "```\n",
    "\n",
    "#### Borrar un fichero desde S3 \n",
    "\n",
    "```python\n",
    "s3.rm(\"my-bucket/my-dir/my-file.txt\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Actividad 2 (en clase)\n",
    "\n",
    "Primero, crea en tu ordenador un fichero local con el contenido que quieras, y nombre `f1.txt`, luego cargalo a tu entorno Jupyter a trav√©s del icono de upload &#x2B06;. Usaremos el mismo bucket creado en la actividad anterior\n",
    "\n",
    "Luego, **Usando el entorno de Jupyter**, ejecuta lo siguiente:\n",
    "\n",
    "- Copia el fichero en tu bucket, bajo el nombre `test/jupyter/file1`\n",
    "- Copia de nuevo el fichero, bajo el nombre `test/jupyter/file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/jupyter/`, deber√≠an aparecer 2 ficheros\n",
    "- Borra el fichero `file2`\n",
    "- Comprueba el contenido de tu bucket bajo la key `test/jupyter/`, deber√≠a aparecer 1 fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(anon=False) # CREATE CLIENT\n",
    "\n",
    "s3.put('mini_input.txt', 's3://mudab-2025-pc1262057/dir3/') # COPIES LOCAL FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mudab-2025-pc1262057/dir3/mini_input.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.ls('s3://mudab-2025-pc1262057/dir3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mudab-2025-pc1262057/dir3/mini_input.txt',\n",
       " 'mudab-2025-pc1262057/dir3/mini_input2.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.put('mini_input.txt', 's3://mudab-2025-pc1262057/dir3/mini_input2.txt')\n",
    "s3.ls('s3://mudab-2025-pc1262057/dir3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mudab-2025-pc1262057/dir3/mini_input.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3.rm('s3://mudab-2025-pc1262057/dir3/mini_input2.txt') # ERASE\n",
    "s3.ls('s3://mudab-2025-pc1262057/dir3/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 3: Big data desde S3 \n",
    "\n",
    "En esta parte, intentaremos procesar la colecci√≥n entera de Tweets que reside en S3. Los pasos a seguir ser√°n los siguientes: \n",
    "\n",
    "1. Desde la consola del Learner Lab, copiar la colecci√≥n entera desde el bucket de origen (`mudab-2025-big-data`) bajo el path `twitter-data`. Los ficheros se denominan `Eurovision-XX.json` donde `XX` es un n√∫mero entre `00` y `09`. Para hacer una copia recursiva de ficheros se puede utilizar el comando de copia de una manera m√°s sofisticada utilizando la opci√≥n `--recursive`. \n",
    "```bash\n",
    "aws s3 cp ‚Äî-recursive s3://mudab-2023/twitter-data/ s3://mudab2023-i123456/input/\n",
    "```\n",
    "\n",
    "2. Utilizar el m√≠smo c√≥digo de la versi√≥n anterior para contestar a las siguientes preguntas: \n",
    "- Cu√°ntos Tweeets son en espa√±ol? \n",
    "- Cual son las 100 palabras m√°s frecuentes en espa√±ol? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5MGuGGFJy2-"
   },
   "source": [
    "## Modelo de Tweets\n",
    "\n",
    "Similar a la clase anterior, creamos un modelo de Tweet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "BF2lAty2c4yy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Tweet:\n",
    "  \"\"\"Class to model a Tweet\"\"\"\n",
    "  id: int         # The unique ID of a tweet\n",
    "  content: str    # The textual content of a tweet\n",
    "  author: str     # The nickname of the author of the tweet\n",
    "  language: str   # The language of the tweet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de input (ETL) \n",
    "\n",
    "Similar a la clase anterior, almacenamos todos los tweets en memoria. \n",
    "\n",
    "**Pregunta**: funcionar√† esto para nuestro input completo? Que hacer si as√≠ no es? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ghv9ixwmcbk9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet(id=995443356309311493, content='RT @jk_rowling: France ‚ù§Ô∏è #Eurovision', author='Liz', language='fr')\n",
      "Tweet(id=995443356602982401, content='RT @neltropico: Salvador Sobral entreg√°ndole el premio a Israel #Eurovision https://t.co/sApzlSoPMb', author='Marina', language='es')\n",
      "Tweet(id=995443356552527873, content='RT @jungjaeguns: cuando apago la luz del pasillo para irme a mi habitaci√≥n y que no me maten los esp√≠ritus #Eurovision https://t.co/0naU3Xm‚Ä¶', author='Rosalinda ‚ô•', language='es')\n",
      "Tweet(id=995443356921720835, content='RT @itsbrookewar: Enrique amigooo!!! \\n#Eurovision https://t.co/HTbbMUwEHd', author='Laura P√©rezüêí', language='es')\n",
      "Tweet(id=995443356825202690, content='RT @snugglycamila: i‚Äôm not surprised eurovision is confusing for americans since the concept of the person with the most votes actually win‚Ä¶', author='Maggi', language='en')\n",
      "Tweet(id=995443357055967233, content='RT @JESSthir: Fuegos artificiales para celebrarlo vamooooooos #eurovision https://t.co/2hwMsyRVAZ', author='Daniel', language='es')\n",
      "Tweet(id=995443357043298304, content='RT @gi_de_off: #Eurovision \\ni just adopted two boys üá∫üá¶‚ù§Ô∏èüá®üáø https://t.co/2ZSvphRPxV', author='hytr_0', language='en')\n",
      "Tweet(id=995443357236252673, content='RT @kkwondragon: No ha ganado una MUJER, no ha ganado una mujer GORDA, no ha ganado una FEMINISTA, no ha ganado un mensaje ANTI-BULLYING. H‚Ä¶', author='ESPARTANA', language='es')\n",
      "Tweet(id=995443356422557696, content='RT @britneyxcheetos: you mean Chub-Bi https://t.co/KijWpZxRYR', author='„Ö§Ÿé', language='en')\n",
      "Tweet(id=995443357366267904, content='Era pra ser a outra colega, mas t√° bom https://t.co/0gvd2ei5aU', author='Gri', language='pt')\n"
     ]
    }
   ],
   "source": [
    "import json, dataclasses\n",
    "\n",
    "tweets = []\n",
    "\n",
    "bucket='mudab-2025-pc1262057'\n",
    "data_key = 'input/Eurovision-00.json'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "\n",
    "def parse_line(line: str):\n",
    "  \"\"\"Try to parse a string into a Person\"\"\"\n",
    "  error = 0\n",
    "  try:\n",
    "    parsed = json.loads(line)\n",
    "    return Tweet(parsed['id'], parsed['text'], parsed['user']['name'], parsed['lang'])\n",
    "  except Exception as e:\n",
    "    error += 1    \n",
    "#   print(f\"Error parsing '{line}': {e}\")\n",
    "\n",
    "with s3.open(data_location) as input: #CLIENT S3. CAN READ THE FILE LINE BY LINE\n",
    "  for line in input.readlines():\n",
    "    if len(line.strip()) > 0:\n",
    "      tweet = parse_line(line)\n",
    "      if tweet: # We add only if the tweet is not 'None'\n",
    "         tweets.append(tweet)\n",
    "\n",
    "for modeled_tweet in tweets[0:10]:\n",
    "  print(modeled_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4PHW8O8wbCO"
   },
   "source": [
    "# Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0M228jcmOWBS",
    "outputId": "8976386a-0645-4be9-a1e0-56bc47767f69",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50874\n",
      "[('RT', 85512), ('#Eurovision', 45318), ('de', 26529), ('a', 23982), ('que', 23961), ('la', 16756), ('y', 14531), ('the', 14414), ('el', 13689), ('en', 11688), ('un', 8159), ('to', 7662), ('Israel', 7521), ('', 7442), ('no', 7327), ('con', 7203), ('una', 6211), ('is', 6209), ('los', 6100), ('#eurovision', 6040), ('por', 5799), ('and', 5562), ('in', 5386), ('a√±o', 5282), ('Eurovision', 5220), ('of', 5200), ('es', 4987), ('for', 4565), ('para', 4427), ('Cuando', 4321), ('lo', 4291), ('se', 4120), ('me', 4087), ('del', 4041), ('ha', 3972), ('I', 3783), ('te', 3769), ('eurovision', 3622), ('gana', 3565), ('you', 3377), ('este', 3324), ('mi', 2886), ('this', 2870), ('A', 2854), ('that', 2836), ('#EUROVISION', 2822), ('su', 2749), ('o', 2742), ('#FinalEurovision', 2672), ('pero', 2548), ('canci√≥n', 2362), ('e', 2353), ('QUE', 2324), ('on', 2307), ('Espa√±a', 2286), ('it', 2281), ('El', 2263), ('le', 2223), ('was', 2222), ('-', 2222), ('al', 2211), ('@ManelNMusic:', 2192), ('las', 2171), ('but', 2140), ('La', 2062), ('yo', 2044), ('cuando', 2039), ('with', 1984), ('winner', 1903), ('gallo', 1872), ('nos', 1810), ('2018', 1804), ('tu', 1791), ('Salvador', 1769), ('so', 1766), ('song', 1751), ('about', 1720), ('pasado', 1719), ('Amaia', 1695), ('not', 1679), ('les', 1652), ('#ESC2018', 1652), ('are', 1646), ('NO', 1636), ('√∫ltimo', 1633), ('esto', 1621), ('mucho', 1618), ('quedo', 1610), ('As√≠', 1596), ('i', 1595), ('han', 1591), ('LA', 1588), ('como', 1580), ('m√°s', 1580), ('da', 1561), ('No', 1560), ('ganado', 1535), ('at', 1527), ('gallina...', 1501), ('https://t.co/EfvXQbb8jp', 1500)]\n"
     ]
    }
   ],
   "source": [
    "import json, dataclasses\n",
    "\n",
    "def read_clean_tweets(input: str):\n",
    "  tweets = []\n",
    "  with open(input, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "  for line in lines:\n",
    "    parsed = json.loads(line)\n",
    "    tweet = Tweet(**parsed)\n",
    "    tweets.append(tweet)\n",
    "  return tweets\n",
    "\n",
    "def count_tweets(language: str, tweets: list[Tweet]):\n",
    "  count = 0\n",
    "  for tweet in tweets:\n",
    "    if (tweet.language == language):\n",
    "      count = count + 1\n",
    "  return count\n",
    "\n",
    "def most_frequent_word(tweets: list[Tweet]):\n",
    "  count = {}\n",
    "  for tweet in tweets:\n",
    "    words = tweet.content.split(' ')\n",
    "    for word in words:\n",
    "      if (word in count):\n",
    "        new_val = count[word] + 1\n",
    "        count[word] = new_val\n",
    "      else:\n",
    "        count[word] = 1\n",
    "  return dict(sorted(count.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "spanish_tweets_count = count_tweets('es', tweets)\n",
    "\n",
    "words_by_frequence = list(most_frequent_word(tweets).items())[0:100]\n",
    "\n",
    "print(spanish_tweets_count)\n",
    "print(words_by_frequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "50874 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwjHMWoRCGn7"
   },
   "source": [
    "# Question 3 (Home, Moodle)\n",
    "\n",
    "Los ejemplos de arriba se han hecho con *un solo* fichero, pero las siguientes preguntas aplican a **TODA** la colecci√≥n (el conjunto de 10 ficheros). \n",
    "\n",
    "- 3.1. Cu√°ntos tweets en espa√±ol son originales (es decir, no son retweets)? \n",
    "- 3.2. Cual es el porcentaje de tweets para cada lenguaje? Es decir, de toda la colleccion, XX% son en idioma YY, ZZ% son en idioma WW, etc...\n",
    "- 3.3. Cu√°les son las palabras m√°s frecuentse en castellano? \n",
    "\n",
    "A√±ade tu respuesta en un bloque de Jupyter aqu√≠ abajo "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
