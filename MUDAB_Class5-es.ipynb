{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f85bb2d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/11 15:51:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/02/11 15:51:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars()))\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    .appName(\"joins\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5762cc2a",
   "metadata": {},
   "source": [
    "# Joins en Spark\n",
    "- solo entre RDD de dos valores (*observación*: los RDD de un valor se pueden transformar a RDDs de dos valores)\n",
    "- la condición sobre la join es siempre de igualdad sobre la clave\n",
    "- sc-> Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89fdd740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Person:\n",
    "    \"\"\"Class to model a Person\"\"\"\n",
    "    name: str\n",
    "    role: str\n",
    "    age: int\n",
    "\n",
    "@dataclass\n",
    "class Address:\n",
    "    \"\"\"Class to model a Person\"\"\"\n",
    "    person_name: str\n",
    "    street: str\n",
    "    number: str\n",
    "    zipcode: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a33db7",
   "metadata": {},
   "source": [
    "Creamos unos datos sinteticos de ejemplo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d5e5b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "person1 = Person(\"Luca\", \"professor\", 25)\n",
    "person2 = Person(\"Ana\", \"coordinator\", 25)\n",
    "\n",
    "address1 = Address(\"Luca\", \"Mallorca\", \"22\", \"12345\")\n",
    "address2 = Address(\"Ana\", \"Paris\", \"33\", \"23456\")\n",
    "address3 = Address(\"Luca\", \"Londres\", \"44\", \"34567\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae92733d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "person_rdd = sc.parallelize([person1, person2])\n",
    "address_rdd = sc.parallelize([address1, address2, address3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76d1bb8",
   "metadata": {},
   "source": [
    "`person_rdd` y `address_rdd` son dos RDD de 1 valor. Ya que join solo funciona para RDDs de dos valores, los transformaremos seleccionando un valor de la clase como clave, y dejando la clase entera como valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "296ad763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "person_rdd_2 = person_rdd.map(lambda x: (x.name, x))\n",
    "address_rdd_2 = address_rdd.map(lambda x: (x.person_name, x))\n",
    "\n",
    "person_address_rdd = person_rdd_2.join(address_rdd_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8b6ee98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Luca', (Person(name='Luca', role='professor', age=25), Address(person_name='Luca', street='Mallorca', number='22', zipcode='12345')))\n",
      "('Luca', (Person(name='Luca', role='professor', age=25), Address(person_name='Luca', street='Londres', number='44', zipcode='34567')))\n",
      "('Ana', (Person(name='Ana', role='coordinator', age=25), Address(person_name='Ana', street='Paris', number='33', zipcode='23456')))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for row in person_address_rdd.collect():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ed3fb9-30aa-4b54-89cc-c399dfb08841",
   "metadata": {},
   "source": [
    "`person_rdd` and `address_rdd`-> creates new RDDs (`person_rdd_2` and `address_rdd_2`) where the elements are tuples with the person as the first element and the object itself as the second element, performs a join operation to match the Person and Address objects based on the name and then prints the resulting tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131151e0",
   "metadata": {},
   "source": [
    "# Un ejemplo más complejo\n",
    "\n",
    "Queremos \"mejorar\" los resultados de nuestro ejemplo anterior de \"Tweet by Language\", imprimiendo en pantalla el nombre completo del idioma (i.e. `English`) en lugar del código de dos caracteres (i.e. `en`). Para alcanzar este objetivo, usaremos un fichero de tipo CSV que contiene la asociación entre código y idioma, disponible en Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a7a44",
   "metadata": {},
   "source": [
    "## Definir los inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05e40d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = 's3a://mudab-2025-big-data/twitter-data/Eurovision-00.json'\n",
    "input_map = 'languages.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6956c68",
   "metadata": {},
   "source": [
    "## Preparar la \"language map\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7c1b3ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "langmap_rdd = sc.textFile(input_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb8754b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Afghanistan,AF,AFG,004,ISO 3166-2:AF,Asia,Southern Asia,\"\",142,034,\"\"\n",
      "Åland Islands,AX,ALA,248,ISO 3166-2:AX,Europe,Northern Europe,\"\",150,154,\"\"\n",
      "Albania,AL,ALB,008,ISO 3166-2:AL,Europe,Southern Europe,\"\",150,039,\"\"\n",
      "Algeria,DZ,DZA,012,ISO 3166-2:DZ,Africa,Northern Africa,\"\",002,015,\"\"\n",
      "American Samoa,AS,ASM,016,ISO 3166-2:AS,Oceania,Polynesia,\"\",009,061,\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for line in langmap_rdd.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa429708",
   "metadata": {},
   "source": [
    "### Observaciones\n",
    "Después de explorar visualmente el fichero, podemos observar que: \n",
    "\n",
    "- En un fichero separado por comas (CSV), con varios campos: \n",
    "    - Country name\n",
    "    - Country 2-letter code \n",
    "    - Country 3-letter code\n",
    "    - ...\n",
    "\n",
    "Que campos necesitamos nosotros para poder hacer el join? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "404dc648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Language(name='Afghanistan', iso_2='af'),\n",
       " Language(name='Åland Islands', iso_2='ax'),\n",
       " Language(name='Albania', iso_2='al'),\n",
       " Language(name='Algeria', iso_2='dz'),\n",
       " Language(name='American Samoa', iso_2='as')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)# frozen is required to be able to hash elements\n",
    "class Language:\n",
    "    name: str\n",
    "    iso_2: str\n",
    "\n",
    "clean_langmap_rdd = (\n",
    "    langmap_rdd\n",
    "    .map(lambda x: x.split(',')) # ARRAY \n",
    "    .map(lambda x: Language(x[0], x[1].lower()))\n",
    ")\n",
    "\n",
    "clean_langmap_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f5a9b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language(name='Afghanistan', iso_2='af')\n",
      "Language(name='Åland Islands', iso_2='ax')\n",
      "Language(name='Albania', iso_2='al')\n",
      "Language(name='Algeria', iso_2='dz')\n",
      "Language(name='American Samoa', iso_2='as')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for line in clean_langmap_rdd.take(5):\n",
    "    print(line)\n",
    "print(clean_langmap_rdd.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29db63fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/02/11 13:58:03 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Tweet:\n",
    "  \"\"\"Class to model a Tweet\"\"\"\n",
    "  id: int         # The unique ID of a tweet\n",
    "  content: str    # The textual content of a tweet\n",
    "  author: str     # The nickname of the author of the tweet\n",
    "  language: str   # The language of the tweet\n",
    "\n",
    "import json\n",
    "\n",
    "def toTweet(line: str):\n",
    "      try:\n",
    "        parsed = json.loads(line)\n",
    "        return Tweet(parsed['id'], parsed['text'], parsed['user']['name'], parsed['lang'])\n",
    "      except Exception as e:\n",
    "        return None\n",
    "\n",
    "rdd = sc.textFile(input_data)\n",
    "\n",
    "processed = (rdd  # rdd[string]\n",
    "    .map(toTweet) # parse string into Tweet. Return rdd[Tweet]\n",
    "    .filter(lambda x: x is not None) # filter empty values. Return same type (rdd[Tweet])\n",
    ")\n",
    "\n",
    "by_lang = (\n",
    "    processed\n",
    "        .map(lambda x: (x.language, 1)) # from rdd[Tweet] to rdd[str, int]\n",
    "        .reduceByKey(lambda x, y: x + y) # reduce produces same type rdd[str, int]\n",
    "        .sortBy(lambda x: x[1], ascending=False) # sort the output. \n",
    "            # Since it's a pair-rdd, select which element of the pair should be used to order \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "634bd6c0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('es', 50874)\n",
      "('en', 30675)\n",
      "('pt', 5831)\n",
      "('fr', 3766)\n",
      "('ru', 2774)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:=====================================================>  (21 + 1) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for line in by_lang.take(5):\n",
    "    print(line)\n",
    "print(by_lang.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706fb4ac",
   "metadata": {},
   "source": [
    "## Join de los RDDs\n",
    "\n",
    "Tenemos ahora dos RDDs de dos valores:\n",
    "- uno contiene la asociación (código, idioma)\n",
    "- el otro contiene la asociación (código, recuento)\n",
    "\n",
    "Cómo hacemos el join?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7ccd560-8787-400a-94a2-1a19f0772a5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('af', 'Afghanistan'), ('ax', 'Åland Islands'), ('al', 'Albania'), ('dz', 'Algeria'), ('as', 'American Samoa')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "langmap_for_join_rdd = clean_langmap_rdd.map(lambda x: (x.iso_2, x.name))\n",
    "print(langmap_for_join_rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "839548fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "joined = by_lang.join(langmap_for_join_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0dc673ea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fr', (3766, 'France'))\n",
      "('it', (2326, 'Italy'))\n",
      "('de', (332, 'Germany'))\n",
      "('sv', (132, 'El Salvador'))\n",
      "('cy', (79, 'Cyprus'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for line in joined.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66af1b3-b422-4726-bfec-7d39704fb178",
   "metadata": {},
   "source": [
    "Reduced by join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee99bf35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final = joined.map(lambda x: (x[1][1], x[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860dae3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 3766)\n",
      "('Italy', 2326)\n",
      "('Germany', 332)\n",
      "('El Salvador', 132)\n",
      "('Cyprus', 79)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for line in final.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5de4990-7f24-4444-9935-42b78e050521",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('France', 3766)\n",
      "('Italy', 2326)\n",
      "('Germany', 332)\n",
      "('El Salvador', 132)\n",
      "('Cyprus', 79)\n",
      "('Suriname', 10)\n",
      "('Argentina', 6)\n",
      "('Russian Federation', 2774)\n",
      "('Ethiopia', 132)\n",
      "('Haiti', 52)\n",
      "('Timor-Leste', 102)\n",
      "('Lithuania', 11)\n",
      "('Romania', 35)\n",
      "('Canada', 286)\n",
      "('Spain', 50874)\n",
      "('Norway', 48)\n",
      "('Latvia', 3)\n",
      "('Poland', 547)\n",
      "('Hungary', 39)\n",
      "('Thailand', 4)\n",
      "('Sierra Leone', 4)\n",
      "('Portugal', 5831)\n",
      "('Finland', 24)\n",
      "('Bulgaria', 7)\n",
      "('Türkiye', 672)\n",
      "('India', 109)\n",
      "('Virgin Islands (U.S.)', 34)\n",
      "('Iceland', 30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "for line in final.collect():\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef50c0f-d963-4239-9d4e-783d6405f618",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d72c3ea-7fbf-4d60-a69a-81214c142646",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preguntas para casa\n",
    "\n",
    "1.1 Cuales son los 10 usarios más mencionados en toda la collecion de Tweets? \n",
    "in the text of the tweet @ markdown\n",
    "\n",
    "1.2 Cuantos tweets escribieron los 10 usuarios más mencionados? \n",
    "\n",
    "Sugerencias: \n",
    "- primero enfocarse en los usuarios más mencionados\n",
    "- luego enfocarse es sacar recuento de tweets por usuario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3d5bb55-28e8-4228-b1d0-3e1ff066e74c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import sagemaker_pyspark\n",
    "import json\n",
    "import re\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c04b0c5c-6d62-4b8d-9b32-a84474279735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK CONFIGURED\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURE SPARK\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.extraClassPath\", \":\".join(sagemaker_pyspark.classpath_jars()))\n",
    "conf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider')\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(conf=conf)\n",
    "    .appName(\"joins\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"SPARK CONFIGURED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27f4881f-acd5-4f65-a5bd-337d86c7aaf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DEFINE A CLASS TO REPRESENT A TWEET\n",
    "@dataclass\n",
    "class Tweet:\n",
    "    \"\"\"CLASS TO MODEL A TWEET\"\"\"\n",
    "    id: int         # THE UNIQUE ID OF A TWEET\n",
    "    content: str    # THE TEXTUAL CONTENT OF A TWEET\n",
    "    author: str     # THE NICKNAME OF THE AUTHOR OF THE TWEET\n",
    "    language: str   # THE LANGUAGE OF THE TWEET\n",
    "\n",
    "# FUNCTION TO PARSE A TWEET FROM A STRING\n",
    "def parse_tweet(line: str):\n",
    "    try:\n",
    "        parsed = json.loads(line)\n",
    "        return Tweet(parsed['id'], parsed['text'], parsed['user']['name'], parsed['lang'])\n",
    "    except Exception as e:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5619681a-a6be-4dbc-a662-c9f854adf5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 USERS:\n",
      "@eurovision: 2264 menciones\n",
      "@manelnmusic: 2218 menciones\n",
      "@nettabarzilai: 2102 menciones\n",
      "@alfred_ot2017: 1025 menciones\n",
      "@amaia_ot2017: 1004 menciones\n",
      "@paquitasalas: 934 menciones\n",
      "@jungjaeguns: 737 menciones\n",
      "@lvpibai: 694 menciones\n",
      "@bbceurovision: 677 menciones\n",
      "@elmundotoday: 630 menciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 103:==================================================>    (20 + 2) / 22]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NUMBER OF TWEETS WRITTEN BY THE TOP 10 USERS:\n",
      "eurovision: 3 Tweet(s)\n",
      "manelnmusic: 0 Tweet(s)\n",
      "nettabarzilai: 0 Tweet(s)\n",
      "alfred_ot2017: 0 Tweet(s)\n",
      "amaia_ot2017: 0 Tweet(s)\n",
      "paquitasalas: 0 Tweet(s)\n",
      "jungjaeguns: 0 Tweet(s)\n",
      "lvpibai: 0 Tweet(s)\n",
      "bbceurovision: 0 Tweet(s)\n",
      "elmundotoday: 0 Tweet(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# FUNCTION TO EXTRACT MENTIONS FROM A TWEET\n",
    "def extract_mentions(tweet: Tweet):\n",
    "    \"\"\"EXTRACT MENTIONS (@USERNAME) FROM THE CONTENT OF THE TWEET.\"\"\"\n",
    "    mentions = re.findall(r'@(\\w+)', tweet.content)  # EXTRACT ONLY VALID USERNAMES\n",
    "    return [(mention.lower(), 1) for mention in mentions]  # CONVERT TO LOWERCASE AND ASSIGN A COUNT\n",
    "\n",
    "# MAIN FUNCTION\n",
    "def main():\n",
    "    input_data = 's3a://mudab-2025-big-data/twitter-data/Eurovision-00.json'\n",
    "    rdd = sc.textFile(input_data)\n",
    "\n",
    "# PROCESS TWEETS\n",
    "    processed = (\n",
    "        rdd  # RDD[STRING]\n",
    "        .map(parse_tweet) # PARSE STRING INTO TWEET. RETURN RDD[TWEET]\n",
    "        .filter(lambda x: x is not None) # FILTER EMPTY VALUES. RETURN SAME TYPE (RDD[TWEET])\n",
    "    )\n",
    "\n",
    "# EXTRACT MENTIONS AND COUNT OCCURRENCES\n",
    "    mentioned_users = (\n",
    "        processed\n",
    "        .flatMap(extract_mentions)  # CONVERT EACH TWEET INTO A LIST OF MENTIONS\n",
    "        .reduceByKey(lambda x, y: x + y)  # SUM MENTIONS BY USER\n",
    "        .sortBy(lambda x: x[1], ascending=False)  # SORT IN DESCENDING ORDER\n",
    "    )\n",
    "\n",
    "\n",
    "# GET THE TOP 10  USERS\n",
    "    top_10_mentions = mentioned_users.take(10)\n",
    "   \n",
    "    print(\"\\nTOP 10 USERS:\")\n",
    "    for user, count in top_10_mentions:\n",
    "        print(f\"@{user}: {count} menciones\")\n",
    "\n",
    "# THEIR TWEETS\n",
    "    top_10_usernames = [user for user, _ in top_10_mentions]\n",
    "    tweets_by_top_users_dict = {user: 0 for user in top_10_usernames}  # Initialize with 0\n",
    "\n",
    "    tweets_by_top_users = (\n",
    "        processed\n",
    "        .filter(lambda tweet: tweet.author.lower() in [user.lower() for user in top_10_usernames])  # FILTER TWEETS BY TOP 10 USERS\n",
    "        .map(lambda tweet: (tweet.author.lower(), 1))  # MAP TWEETS TO (USER, 1)\n",
    "        .reduceByKey(lambda x, y: x + y)  # SUM TWEETS BY USER\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "# DICTIONARY\n",
    "    for user, count in tweets_by_top_users:\n",
    "        tweets_by_top_users_dict[user] = count\n",
    "\n",
    "    print(\"\\nNUMBER OF TWEETS WRITTEN BY THE TOP 10 USERS:\")\n",
    "    for user, count in sorted(tweets_by_top_users_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{user}: {count} Tweet(s)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11201cb5-fde4-4ba4-9a9a-2e5f22485170",
   "metadata": {
    "tags": []
   },
   "source": [
    "### If we wanted the whole collection of tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7959c90-5caf-412c-973c-4469873c7671",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TOP 10 USERS:\n",
      "@eurovision: 25754 menciones\n",
      "@paquitasalas: 16446 menciones\n",
      "@bbceurovision: 14442 menciones\n",
      "@alfred_ot2017: 14379 menciones\n",
      "@amaia_ot2017: 13762 menciones\n",
      "@nettabarzilai: 11087 menciones\n",
      "@netflixes: 10859 menciones\n",
      "@manelnmusic: 9794 menciones\n",
      "@pewdiepie: 7194 menciones\n",
      "@elmundotoday: 6929 menciones\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:====================================================>(216 + 3) / 220]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NUMBER OF TWEETS WRITTEN BY THE TOP 10 USERS:\n",
      "eurovision: 84 Tweet(s)\n",
      "pewdiepie: 5 Tweet(s)\n",
      "paquitasalas: 0 Tweet(s)\n",
      "bbceurovision: 0 Tweet(s)\n",
      "alfred_ot2017: 0 Tweet(s)\n",
      "amaia_ot2017: 0 Tweet(s)\n",
      "nettabarzilai: 0 Tweet(s)\n",
      "netflixes: 0 Tweet(s)\n",
      "manelnmusic: 0 Tweet(s)\n",
      "elmundotoday: 0 Tweet(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# FUNCTION TO EXTRACT MENTIONS FROM A TWEET\n",
    "def extract_mentions(tweet: Tweet):\n",
    "    \"\"\"EXTRACT MENTIONS (@USERNAME) FROM THE CONTENT OF THE TWEET.\"\"\"\n",
    "    mentions = re.findall(r'@(\\w+)', tweet.content)  # EXTRACT ONLY VALID USERNAMES\n",
    "    return [(mention.lower(), 1) for mention in mentions]  # CONVERT TO LOWERCASE AND ASSIGN A COUNT\n",
    "\n",
    "# MAIN FUNCTION\n",
    "def main():\n",
    "    input_data = 's3a://mudab-2025-big-data/twitter-data/Eurovision-*.json'\n",
    "    rdd = sc.textFile(input_data)\n",
    "\n",
    "# PROCESS TWEETS\n",
    "    processed = (\n",
    "        rdd  # RDD[STRING]\n",
    "        .map(parse_tweet) # PARSE STRING INTO TWEET. RETURN RDD[TWEET]\n",
    "        .filter(lambda x: x is not None) # FILTER EMPTY VALUES. RETURN SAME TYPE (RDD[TWEET])\n",
    "    )\n",
    "\n",
    "# EXTRACT MENTIONS AND COUNT OCCURRENCES\n",
    "    mentioned_users = (\n",
    "        processed\n",
    "        .flatMap(extract_mentions)  # CONVERT EACH TWEET INTO A LIST OF MENTIONS\n",
    "        .reduceByKey(lambda x, y: x + y)  # SUM MENTIONS BY USER\n",
    "        .sortBy(lambda x: x[1], ascending=False)  # SORT IN DESCENDING ORDER\n",
    "    )\n",
    "\n",
    "\n",
    "# GET THE TOP 10  USERS\n",
    "    top_10_mentions = mentioned_users.take(10)\n",
    "   \n",
    "    print(\"\\nTOP 10 USERS:\")\n",
    "    for user, count in top_10_mentions:\n",
    "        print(f\"@{user}: {count} menciones\")\n",
    "\n",
    "# THEIR TWEETS\n",
    "    top_10_usernames = [user for user, _ in top_10_mentions]\n",
    "    tweets_by_top_users_dict = {user: 0 for user in top_10_usernames}  # Initialize with 0\n",
    "\n",
    "    tweets_by_top_users = (\n",
    "        processed\n",
    "        .filter(lambda tweet: tweet.author.lower() in [user.lower() for user in top_10_usernames])  # FILTER TWEETS BY TOP 10 USERS\n",
    "        .map(lambda tweet: (tweet.author.lower(), 1))  # MAP TWEETS TO (USER, 1)\n",
    "        .reduceByKey(lambda x, y: x + y)  # SUM TWEETS BY USER\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "# DICTIONARY\n",
    "    for user, count in tweets_by_top_users:\n",
    "        tweets_by_top_users_dict[user] = count\n",
    "\n",
    "    print(\"\\nNUMBER OF TWEETS WRITTEN BY THE TOP 10 USERS:\")\n",
    "    for user, count in sorted(tweets_by_top_users_dict.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"{user}: {count} Tweet(s)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
